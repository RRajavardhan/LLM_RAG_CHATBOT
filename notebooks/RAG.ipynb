{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMK9irwh1Bns"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install -U torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "ACCESS_TOKEN = os.getenv(\"ACCESS_TOKEN\") # reads .env file with ACCESS_TOKEN=<your hugging face access token>\n",
        "\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=ACCESS_TOKEN)\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_use_double_quant=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             token=ACCESS_TOKEN)\n",
        "model.eval()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"using {device}\")"
      ],
      "metadata": {
        "id": "HTm9GEta10Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(question: str, context: str):\n",
        "\n",
        "    if context == None or context == \"\":\n",
        "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
        "            Context: {context}.\n",
        "            Question: {question}\"\"\"\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        # { \"role\": \"model\", \"content\": \"Recurrent Attention (RAG)** is a novel neural network architecture specifically designed\" }\n",
        "    ]\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "    inputs = tokenizer.encode(\n",
        "        formatted_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=250,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    response = response[len(formatted_prompt) :]  # remove input prompt from reponse\n",
        "    response = response.replace(\"<eos>\", \"\")  # remove eos token\n",
        "    return response\n",
        "\n",
        "\n",
        "question = \"What is a transformer?\"\n",
        "print(inference(question=question, context=\"\"))"
      ],
      "metadata": {
        "id": "KuxRcT_71z9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "3kUI2WQn1z6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loaders = [\n",
        "    PyPDFLoader(\"/home/eversberg/Downloads/1706.03762.pdf\"),\n",
        "    PyPDFLoader(\"/home/eversberg/Downloads/2005.11401.pdf\"),\n",
        "]\n",
        "pages = []\n",
        "for loader in loaders:\n",
        "    pages.extend(loader.load())"
      ],
      "metadata": {
        "id": "7SjPGDz-1z3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "ECZYfjJo1z1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "8gGK9JCO1zyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "fgSVxpeS1zv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain_community.embeddings import (\n",
        "    HuggingFaceEmbeddings\n",
        ")\n",
        "encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})"
      ],
      "metadata": {
        "id": "u1iTyy7j1ztP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings1 = encoder.embed_query(\"RAG\")\n",
        "embeddings2 = encoder.embed_query(docs[0].page_content)\n",
        "print(np.dot(embeddings1, embeddings2))"
      ],
      "metadata": {
        "id": "AV_o6TKU1zq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "zNuGKaZD1zof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "faiss_db = FAISS.from_documents(docs, encoder, distance_strategy=DistanceStrategy.DOT_PRODUCT)"
      ],
      "metadata": {
        "id": "yFxbBKni1zlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is a transformer?\"\n",
        "retrieved_docs = faiss_db.similarity_search(question, k=5)\n",
        "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
        "print(context)"
      ],
      "metadata": {
        "id": "8D67_-Dv1zji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(inference(question=question, context=context))"
      ],
      "metadata": {
        "id": "oOM5QdEt1zhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For this answer I used the following documents:\")\n",
        "for doc in retrieved_docs:\n",
        "    print(doc.metadata)"
      ],
      "metadata": {
        "id": "0ErAZBB41ze9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}